{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d50d1b4",
   "metadata": {},
   "source": [
    "Load the \"MainProcess.xes\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "import pandas as pd\n",
    "\n",
    "# Directory with your .xes files\n",
    "xes_directory = os.path.join(os.getcwd(), \"20130794\", \"Cleaned Event Log\")\n",
    "\n",
    "# Output directory for models or visuals\n",
    "output_directory = os.path.join(os.getcwd(), \"output\")\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process MainProcess.xes file\n",
    "filename = \"MainProcess.xes\"\n",
    "file_path = os.path.join(xes_directory, filename)\n",
    "print(f\"Processing {filename}\")\n",
    "\n",
    "try:\n",
    "    log = xes_importer.apply(file_path)\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Imported {filename} with {len(log)} traces.\")\n",
    "    \n",
    "    # Print important information about the log\n",
    "    print(f\"Number of events: {sum(len(trace) for trace in log)}\")\n",
    "    activities = set(event[\"concept:name\"] for trace in log for event in trace if \"concept:name\" in event)\n",
    "    print(f\"Number of unique activities: {len(activities)}\")\n",
    "    print(f\"Unique activities: {activities}\")\n",
    "    case_ids = [trace.attributes[\"concept:name\"] for trace in log if \"concept:name\" in trace.attributes]\n",
    "    print(f\"Number of cases: {len(case_ids)}\")\n",
    "    print(f\"First 5 case IDs: {case_ids[:5]}\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932548e6",
   "metadata": {},
   "source": [
    "Get all the attributes included in this file (\"MainProcess.xes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # List all attributes of the traces in a table\n",
    "    print(\"\\n\\nAttributes of traces:\\n\")\n",
    "    \n",
    "    trace_attributes = set()\n",
    "    for trace in log:\n",
    "        trace_attributes.update(trace.attributes.keys())\n",
    "        \n",
    "    trace_attr_info = []\n",
    "    for attr in trace_attributes:\n",
    "        attr_type = \"unknown\"\n",
    "        for trace in log:\n",
    "            if attr in trace.attributes:\n",
    "                attr_type = type(trace.attributes[attr]).__name__\n",
    "                break\n",
    "        trace_attr_info.append({\"Attribute\": attr, \"Type\": attr_type})\n",
    "\n",
    "    trace_attr_df = pd.DataFrame(trace_attr_info)\n",
    "    display(trace_attr_df)\n",
    "    # Save the attribute DataFrame to an Excel file in a \"tables\" subfolder\n",
    "    tables_dir = os.path.join(os.getcwd(), \"tables\")\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    trace_attr_df.to_excel(os.path.join(tables_dir, \"main_trace_attribute_info.xlsx\"), index=False)\n",
    "    \n",
    "    # List all attributes of the events in a table\n",
    "    print(\"\\n\\nAttributes of events:\\n\")\n",
    "    \n",
    "    event_attributes = set()\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            event_attributes.update(event.keys())\n",
    "\n",
    "    event_attr_info = []\n",
    "    for attr in event_attributes:\n",
    "        attr_type = \"unknown\"\n",
    "        for trace in log:\n",
    "            for event in trace:\n",
    "                if attr in event:\n",
    "                    attr_type = type(event[attr]).__name__\n",
    "                    break\n",
    "            if attr_type != \"unknown\":\n",
    "                break\n",
    "        event_attr_info.append({\"Attribute\": attr, \"Type\": attr_type})\n",
    "\n",
    "    event_attr_df = pd.DataFrame(event_attr_info)\n",
    "    display(event_attr_df)\n",
    "    # Save the attribute DataFrame to an Excel file in a \"tables\" subfolder\n",
    "    tables_dir = os.path.join(os.getcwd(), \"tables\")\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    event_attr_df.to_excel(os.path.join(tables_dir, \"main_event_attribute_info.xlsx\"), index=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing the log: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c4d128",
   "metadata": {},
   "source": [
    "List all the resources of the log in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set pandas display options for full width\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.set_option('display.width', 0)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    resources = set()\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            if \"org:resource\" in event:\n",
    "                resources.add(event[\"org:resource\"])\n",
    "\n",
    "    resource_info = []\n",
    "    for resource in resources:\n",
    "        # Find the first SubProcessID for this resource by searching events until found\n",
    "        first_subprocess_id = \"N/A\"\n",
    "        parameters_dict = None\n",
    "        found = False\n",
    "        for trace in log:\n",
    "                for event in trace:\n",
    "                    if event.get(\"org:resource\") == resource:\n",
    "                        if \"SubProcessID\" in event:\n",
    "                            first_subprocess_id = event[\"SubProcessID\"]\n",
    "                            found = True\n",
    "                            break\n",
    "                if found:\n",
    "                    break\n",
    "                \n",
    "        found = False\n",
    "        for trace in log:\n",
    "                for event in trace:\n",
    "                    if event.get(\"org:resource\") == resource:\n",
    "                        if \"parameters\" in event:\n",
    "                            parameters_dict = event[\"parameters\"]\n",
    "                            found = True\n",
    "                            break\n",
    "                if found:\n",
    "                    break\n",
    "        event_count = sum(1 for trace in log for event in trace if event.get(\"org:resource\") == resource)\n",
    "        activities_performed = set(event[\"concept:name\"] for trace in log for event in trace if event.get(\"org:resource\") == resource)\n",
    "        parameter_keys = list(parameters_dict['children'][i][0] for i in range(len(parameters_dict['children']))) if parameters_dict and 'children' in parameters_dict else []\n",
    "        resource_info.append({\n",
    "            \"Resource\": resource,\n",
    "            \"Event Count\": event_count,\n",
    "            \"Unique Activities\": len(activities_performed),\n",
    "            \"Activities\": \", \".join(sorted(activities_performed)),\n",
    "            \"First Subprocess ID\": first_subprocess_id,\n",
    "            \"Parameter Keys\": parameter_keys\n",
    "        })\n",
    "\n",
    "    resource_df = pd.DataFrame(resource_info)\n",
    "    display(resource_df)\n",
    "    # Save the attribute DataFrame to an Excel file in a \"tables\" subfolder\n",
    "    tables_dir = os.path.join(os.getcwd(), \"tables\")\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    resource_df.to_excel(os.path.join(tables_dir, \"resources_info.xlsx\"), index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing the log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0cc698",
   "metadata": {},
   "source": [
    "Load a single subevent log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18723d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process 0a0a7c16-85d9-48be-a7d5-32931240c337.xes file\n",
    "filename = \"0a0a7c16-85d9-48be-a7d5-32931240c337.xes\"\n",
    "file_path = os.path.join(xes_directory, filename)\n",
    "print(f\"Processing {filename}\")\n",
    "\n",
    "try:\n",
    "    subevent_log = xes_importer.apply(file_path)\n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Imported {filename} with {len(subevent_log)} traces.\")\n",
    "\n",
    "    # Print important information about the subevent_log\n",
    "    print(f\"Number of events: {sum(len(trace) for trace in subevent_log)}\")\n",
    "    activities = set(event[\"concept:name\"] for trace in subevent_log for event in trace if \"concept:name\" in event)\n",
    "    print(f\"Number of unique activities: {len(activities)}\")\n",
    "    print(f\"Unique activities: {activities}\")\n",
    "    case_ids = [trace.attributes[\"concept:name\"] for trace in subevent_log if \"concept:name\" in trace.attributes]\n",
    "    print(f\"Number of cases: {len(case_ids)}\")\n",
    "    print(f\"First 5 case IDs: {case_ids[:5]}\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd166157",
   "metadata": {},
   "source": [
    "List all the attributes in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbb508",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # List all attributes in the subevent_log in a table and display nicely in Jupyter Notebook\n",
    "    all_attributes = set()\n",
    "    for trace in subevent_log:\n",
    "        all_attributes.update(trace.attributes.keys())\n",
    "        for event in trace:\n",
    "            all_attributes.update(event.keys())\n",
    "\n",
    "    # Prepare attribute type information\n",
    "    attr_info = []\n",
    "    for attr in all_attributes:\n",
    "        if attr in subevent_log[0].attributes:\n",
    "            attr_type = type(subevent_log[0].attributes[attr]).__name__\n",
    "        elif len(subevent_log[0]) > 0 and attr in subevent_log[0][0]:\n",
    "            attr_type = type(subevent_log[0][0][attr]).__name__\n",
    "        else:\n",
    "            attr_type = \"unknown\"\n",
    "        attr_info.append({\"Attribute\": attr, \"Type\": attr_type})\n",
    "\n",
    "    # Display as a pandas DataFrame\n",
    "    attr_df = pd.DataFrame(attr_info)\n",
    "    display(attr_df)\n",
    "    # Save the attribute DataFrame to an Excel file in a \"tables\" subfolder\n",
    "    tables_dir = os.path.join(os.getcwd(), \"tables\")\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    attr_df.to_excel(os.path.join(tables_dir, \"sub_attribute_info.xlsx\"), index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing the subevent_log: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddbb29",
   "metadata": {},
   "source": [
    "Get all the sensor data included per resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c465224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from IPython.display import display\n",
    "\n",
    "# Helper: parse datastream list element into a dict structure\n",
    "def parse_datastream_from_event_xml(event_xml):\n",
    "    # Namespace for XES\n",
    "    ns = {\"xes\": \"http://code.deckfour.org/xes\"}\n",
    "    # datastream list(s) under this event\n",
    "    datastreams = event_xml.findall(\"xes:list[@key='stream:datastream']\", ns)\n",
    "    if not datastreams:\n",
    "        return None\n",
    "\n",
    "    # We'll collect multiple datastream lists if present (usually one). For simplicity,\n",
    "    # if more than one datastream list exists we'll merge their children.\n",
    "    merged = {\"children\": {}}\n",
    "    for datastream in datastreams:\n",
    "        # Each 'point' is a nested list inside the datastream\n",
    "        for idx, point in enumerate(datastream.findall(\"xes:list\", ns)):\n",
    "            # create sensor_point with attributes from the point element (attributes are in the 'stream' namespace)\n",
    "            # the stream namespace is: https://cpee.org/datastream/datastream.xesext\n",
    "            stream_ns = \"https://cpee.org/datastream/datastream.xesext\"\n",
    "            sensor_point = {\n",
    "                \"stream:system\": point.attrib.get(f\"{{{stream_ns}}}system\"),\n",
    "                \"stream:system_type\": point.attrib.get(f\"{{{stream_ns}}}system_type\"),\n",
    "                \"stream:observation\": point.attrib.get(f\"{{{stream_ns}}}observation\"),\n",
    "                \"stream:procedure_type\": point.attrib.get(f\"{{{stream_ns}}}procedure_type\"),\n",
    "                \"stream:interaction_type\": point.attrib.get(f\"{{{stream_ns}}}interaction_type\"),\n",
    "                \"children\": {}\n",
    "            }\n",
    "\n",
    "            # child elements inside the point (like <date key=\"stream:timestamp\" value=\"...\"/>)\n",
    "            for child in point:\n",
    "                # child.attrib typically has 'key' and 'value'\n",
    "                key = child.attrib.get(\"key\")\n",
    "                val = child.attrib.get(\"value\")\n",
    "                if key:\n",
    "                    sensor_point[\"children\"][key] = val\n",
    "\n",
    "            # name the sensor node: use the 'key' attribute of the point element if available,\n",
    "            # otherwise create an indexed name to keep them unique.\n",
    "            sensor_key = point.attrib.get(\"key\", f\"stream:point_{idx}\")\n",
    "            # If same key already exists, create an indexed unique name\n",
    "            if sensor_key in merged[\"children\"]:\n",
    "                # find a unique postfix\n",
    "                i = 1\n",
    "                new_key = f\"{sensor_key}_{i}\"\n",
    "                while new_key in merged[\"children\"]:\n",
    "                    i += 1\n",
    "                    new_key = f\"{sensor_key}_{i}\"\n",
    "                sensor_key = new_key\n",
    "\n",
    "            merged[\"children\"][sensor_key] = sensor_point\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Helper: attach parsed sensor dicts to pm4py events (in-memory)\n",
    "def attach_sensor_data_to_pm4py_log(pm4py_log, xes_file_path):\n",
    "    ns = {\"xes\": \"http://code.deckfour.org/xes\"}\n",
    "    tree = ET.parse(xes_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    xml_traces = root.findall(\".//xes:trace\", ns)\n",
    "\n",
    "    if len(xml_traces) != len(pm4py_log):\n",
    "        print(f\"Warning: number of XML traces ({len(xml_traces)}) != number of pm4py traces ({len(pm4py_log)}). \"\n",
    "              \"We'll attach for the minimum of both and skip extras.\")\n",
    "\n",
    "    n_traces = min(len(xml_traces), len(pm4py_log))\n",
    "\n",
    "    for t_idx in range(n_traces):\n",
    "        xml_trace = xml_traces[t_idx]\n",
    "        pm_trace = pm4py_log[t_idx]\n",
    "\n",
    "        xml_events = xml_trace.findall(\"xes:event\", ns)\n",
    "        pm_events = list(pm_trace)\n",
    "\n",
    "        if len(xml_events) != len(pm_events):\n",
    "            # warn but still attach for min length (common case: may still match)\n",
    "            print(f\"Warning: trace {t_idx} has {len(xml_events)} XML events vs {len(pm_events)} pm4py events. \"\n",
    "                  \"Attaching up to the minimum matched events in order.\")\n",
    "\n",
    "        n_events = min(len(xml_events), len(pm_events))\n",
    "        for e_idx in range(n_events):\n",
    "            xml_event = xml_events[e_idx]\n",
    "            pm_event = pm_events[e_idx]\n",
    "\n",
    "            datastream_dict = parse_datastream_from_event_xml(xml_event)\n",
    "            if datastream_dict:\n",
    "                # attach the Python dict directly into the pm4py event\n",
    "                pm_event[\"stream:datastream\"] = datastream_dict\n",
    "\n",
    "    # returns pm4py_log mutated in place\n",
    "    return pm4py_log\n",
    "\n",
    "def get_sensor_data_from_subfile(subprocess_log):\n",
    "    sensor_dict = {}\n",
    "    \n",
    "    # Go through all events and extract sensor data if present\n",
    "    for trace in subprocess_log:\n",
    "        for event in trace:\n",
    "            if \"stream:datastream\" in event:\n",
    "                sensor_data = event[\"stream:datastream\"]\n",
    "                resource_name = event.get(\"org:resource\", \"Unknown Resource\")\n",
    "                sensor_info = []\n",
    "                \n",
    "                # sensor_data is a dict with 'children' key containing sensor points\n",
    "                if \"children\" in sensor_data:\n",
    "                    for sensor_name, sensor_point in sensor_data[\"children\"].items():\n",
    "                        # sensor_name is usually 'stream:point' or similar\n",
    "                        if isinstance(sensor_point, dict) and \"children\" in sensor_point:\n",
    "                            sensor_details = sensor_point[\"children\"]\n",
    "                            timestamp = sensor_details.get(\"stream:timestamp\")\n",
    "                            value = sensor_details.get(\"stream:value\")\n",
    "                            system = sensor_point.get(\"stream:system\")\n",
    "                            system_type = sensor_point.get(\"stream:system_type\")\n",
    "                            observation = sensor_point.get(\"stream:observation\")\n",
    "                            procedure_type = sensor_point.get(\"stream:procedure_type\")\n",
    "                            interaction_type = sensor_point.get(\"stream:interaction_type\")\n",
    "                            sensor_info.append({\n",
    "                                \"Sensor Name\": sensor_name,\n",
    "                                \"System\": system,\n",
    "                                \"System Type\": system_type,\n",
    "                                \"Observation\": observation,\n",
    "                                \"Procedure Type\": procedure_type,\n",
    "                                \"Interaction Type\": interaction_type,\n",
    "                                \"Timestamp\": timestamp,\n",
    "                                \"Value\": value,\n",
    "                                \"Resouce\": resource_name\n",
    "                            })\n",
    "                            \n",
    "    return sensor_info\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main loop (your original structure, adapted)\n",
    "# -------------------------\n",
    "# resource_df is assumed to be defined elsewhere (DataFrame with Resource and First Subprocess ID columns)\n",
    "# xes_directory likewise defined\n",
    "\n",
    "# sensor_dict is used as a mapping from resource -> DataFrame, so initialize as a dict\n",
    "sensor_dict = {}\n",
    "\n",
    "xes_files = [\n",
    "    f for f in os.listdir(xes_directory)\n",
    "    if f.endswith('.xes') and f != \"MainProcess.xes\"\n",
    "]\n",
    "\n",
    "sensor_df = None\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def process_xes_file(filename):\n",
    "    try:\n",
    "        print(f\"Found XES file: {filename}\")\n",
    "        xes_path = os.path.join(xes_directory, filename)\n",
    "        if not os.path.isfile(xes_path):\n",
    "            print(f\"Error: File not found: {xes_path}\")\n",
    "            return None\n",
    "        try:\n",
    "            subprocess_log = xes_importer.apply(xes_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error importing XES file {filename}: {e}\")\n",
    "            return None\n",
    "        try:\n",
    "            attach_sensor_data_to_pm4py_log(subprocess_log, xes_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error attaching sensor data for {filename}: {e}\")\n",
    "            return None\n",
    "        try:\n",
    "            sensor_info = get_sensor_data_from_subfile(subprocess_log)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting sensor data from {filename}: {e}\")\n",
    "            return None\n",
    "        if sensor_info:\n",
    "            try:\n",
    "                sub_sensor_df = pd.DataFrame(sensor_info)\n",
    "                # Add a column with the count of how often each sensor (by \"System\") appeared before grouping\n",
    "                sub_sensor_df[\"Count\"] = sub_sensor_df.groupby(\"System\")[\"System\"].transform(\"count\")\n",
    "                sub_sensor_df = sub_sensor_df.groupby(\"System\", as_index=False).first()\n",
    "                return sub_sensor_df\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating DataFrame for {filename}: {e}\")\n",
    "                return None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "sensor_df = None\n",
    "# Use ThreadPoolExecutor instead of ProcessPoolExecutor for Jupyter compatibility\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_xes_file, xes_files))\n",
    "    for sub_sensor_df in results:\n",
    "        if sub_sensor_df is not None:\n",
    "            if sensor_df is None:\n",
    "                sensor_df = sub_sensor_df\n",
    "            else:\n",
    "                sensor_df = pd.concat([sensor_df, sub_sensor_df], ignore_index=True)\n",
    "    \n",
    "if sensor_df is not None:\n",
    "    sensor_df = sensor_df.groupby(\"System\", as_index=False).agg({\n",
    "        **{col: \"first\" for col in sensor_df.columns if col not in [\"System\", \"Count\"]},\n",
    "        \"Count\": \"sum\"\n",
    "    })\n",
    "    display(sensor_df)\n",
    "    # Save the combined sensor DataFrame to an Excel file in a \"tables\" subfolder\n",
    "    tables_dir = os.path.join(os.getcwd(), \"tables\")\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    sensor_df.to_excel(os.path.join(tables_dir, \"combined_sensor_data.xlsx\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e856a9",
   "metadata": {},
   "source": [
    "Use the sensor dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb851d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sensor_dict))\n",
    "\n",
    "for resource, sensor_df in sensor_dict.items():\n",
    "    print(f\"\\nSensor data for Resource: {resource}\\n\")\n",
    "    sensor_df = sensor_df.drop(columns=[\"Interaction Type\", \"Timestamp\"], errors=\"ignore\")\n",
    "    sensor_df = sensor_df.groupby(\"System\").agg(lambda x: x.iloc[0]).reset_index()\n",
    "    display(sensor_df)\n",
    "    # Save the sensor DataFrame to an Excel file in a \"tables\" subfolder\n",
    "    tables_dir = os.path.join(os.getcwd(), \"tables\")\n",
    "    os.makedirs(tables_dir, exist_ok=True)\n",
    "    sensor_df.to_excel(os.path.join(tables_dir, f\"sensor_data_{resource}.xlsx\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
